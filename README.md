cat <<EOF > README.md
# ğŸ“„ QAWithPDF - Chat with Documents using Gemini 3.0

**QAWithPDF** is a Retrieval-Augmented Generation (RAG) application that allows users to upload PDF documents and ask questions about their content in natural language. 

Built with **Streamlit**, **LlamaIndex**, and **Google Gemini** (Flash models), this project demonstrates a modern, modular approach to building AI-powered QA systems with robust error handling and logging.

## ğŸš€ Features

* **ğŸ“„ PDF Ingestion:** Upload and process PDF documents instantly.
* **ğŸ§  Advanced RAG Pipeline:** Uses LlamaIndex for efficient vector storage and retrieval.
* **âš¡ Powered by Gemini:** Utilizes Google's **Gemini 2.0 Flash / 3.0 Flash Preview** for high-speed, accurate responses.
* **ğŸ” Custom Embeddings:** Uses \`text-embedding-004\` for high-quality vector representations.
* **ğŸ› ï¸ Production-Grade Engineering:** Includes custom logging, exception handling, and modular code architecture.
* **ğŸ’» Interactive UI:** User-friendly interface built with Streamlit.

---

## ğŸ“‚ Project Structure

\`\`\`text
QAGEMINI/
â”œâ”€â”€ QAWithPDF/               # Core Application Logic
â”‚   â”œâ”€â”€ __init__.py          # Package initialization
â”‚   â”œâ”€â”€ data_ingestion.py    # Handles loading and processing PDFs
â”‚   â”œâ”€â”€ embedding.py         # Manages VectorStore and Embeddings
â”‚   â””â”€â”€ model_api.py         # Configures the Gemini LLM
â”œâ”€â”€ Experiments/             # Testing Playground
â”‚   â””â”€â”€ experiment.ipynb     # Jupyter notebook for testing logic
â”œâ”€â”€ logs/                    # Runtime logs (Auto-generated)
â”œâ”€â”€ Data/                    # Temporary storage for uploaded files
â”œâ”€â”€ storage/                 # Local Vector Database persistence
â”œâ”€â”€ StreamlitApp.py          # Main Frontend Application
â”œâ”€â”€ logger.py                # Custom Logging Configuration
â”œâ”€â”€ exception.py             # Custom Exception Handling
â”œâ”€â”€ setup.py                 # Package Setup
â”œâ”€â”€ requirements.txt         # Project Dependencies
â”œâ”€â”€ .env                     # API Keys (Not tracked in Git)
â””â”€â”€ .gitignore               # Git Ignore Rules
\`\`\`

---

## ğŸ› ï¸ Installation & Setup

### 1. Clone the Repository
\`\`\`bash
git clone <repository-url>
cd QAGEMINI
\`\`\`

### 2. Create a Virtual Environment (Recommended)
\`\`\`bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
\`\`\`

### 3. Install Dependencies
\`\`\`bash
pip install -r requirements.txt
\`\`\`

### 4. Configure API Keys
Create a \`.env\` file in the root directory and add your Google API Key:
\`\`\`env
GOOGLE_API_KEY=your_google_api_key_here
\`\`\`
> **Note:** Get your free API key from [Google AI Studio](https://aistudio.google.com/).

---

## â–¶ï¸ How to Run

To start the application, run the Streamlit frontend:

\`\`\`bash
streamlit run StreamlitApp.py
\`\`\`

The application will open in your browser at \`http://localhost:8501\`.

1.  **Upload** a PDF document using the sidebar or upload area.
2.  Click **"Submit & Process"** to ingest the file.
3.  Type your question in the text box (e.g., *"Summarize this document"* or *"What is the main conclusion?"*).
4.  Wait for Gemini to generate the answer!

---

## ğŸ”§ Technical Details

* **LLM:** Google Gemini 2.0 Flash / 3.0 Flash Preview (via \`llama-index-llms-gemini\`)
* **Embeddings:** Google \`text-embedding-004\`
* **Orchestration:** LlamaIndex (v0.10+)
* **Frontend:** Streamlit
* **Vector Store:** Local File Storage (Persisted via LlamaIndex \`StorageContext\`)

---

## ğŸ”® Future Improvements
* Add **Chat Memory** to support follow-up questions.
* Integrate **LlamaParse** for better handling of tables and complex formatting.
* Deploy to **Streamlit Cloud** or **Hugging Face Spaces**.
* Migrate vector storage to a cloud vector database (e.g., Pinecone or Weaviate).

---

## ğŸ“ License
This project is open-source and available under the MIT License.
EOF